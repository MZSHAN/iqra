{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOC0BxHbaqlk",
        "outputId": "58d8ce5d-a16d-49fa-c64b-e294771234ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4CpwNZCavVA",
        "outputId": "fb36f59d-a513-4b65-de1d-a4626954f196"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  9 08:05:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0             26W /   70W |     120MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_multiplication.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define CHECK_CUDA_CALL(err)                                                \\\n",
        "    {                                                                       \\\n",
        "        if (err != cudaSuccess)                                             \\\n",
        "        {                                                                   \\\n",
        "            fprintf(stderr, \"CUDA error in file %s at line %d: %s\\n\",       \\\n",
        "                    __FILE__, __LINE__, cudaGetErrorString(err));           \\\n",
        "            exit(EXIT_FAILURE);                                             \\\n",
        "        }                                                                   \\\n",
        "    }\n",
        "\n",
        "#define TILE_SIZE 32\n",
        "#define COARSE_FACTOR 4\n",
        "// Reduce number of blocks and lower number of blocks can run parallely - few threads handle more \"units\" of work\n",
        "\n",
        "__global__\n",
        "void matrix_multiplication_tiled_thread_coarsed(float *A, float* B, float *result, int rows_result, int col_result, int inner_dim){\n",
        "    __shared__ float A_ds[TILE_SIZE][TILE_SIZE];\n",
        "    __shared__ float B_ds[TILE_SIZE][TILE_SIZE];\n",
        "\n",
        "    int bx = blockIdx.x; int tx = threadIdx.x;\n",
        "    int by = blockIdx.y; int ty = threadIdx.y;\n",
        "\n",
        "    int row = by * TILE_SIZE + ty;\n",
        "    int col_offset = bx * TILE_SIZE * COARSE_FACTOR;\n",
        "\n",
        "    float dot_product[COARSE_FACTOR];\n",
        "    for (int i=0; i< COARSE_FACTOR; ++i){\n",
        "        dot_product[i] = 0.0;\n",
        "    }\n",
        "    // Loop over all tiles\n",
        "    for (int ph=0; ph < (inner_dim + TILE_SIZE - 1)/TILE_SIZE; ++ph ){\n",
        "\n",
        "        //r = row, c = TILE_SIZE * ph + tx;\n",
        "        if (row < rows_result && (TILE_SIZE*ph + tx) < inner_dim){\n",
        "            A_ds[ty][tx] = A[row * inner_dim + TILE_SIZE * ph + tx];\n",
        "        }\n",
        "        else{\n",
        "            A_ds[ty][tx] = 0.0;\n",
        "        }\n",
        "\n",
        "        for (int coarse_idx=0; coarse_idx<COARSE_FACTOR; ++coarse_idx){\n",
        "            int col = col_offset + TILE_SIZE * coarse_idx + tx;\n",
        "            //row = TILE_SIZE * ph + ty, c = col\n",
        "            if ((TILE_SIZE * ph + ty) < inner_dim && col < col_result){\n",
        "                B_ds[ty][tx] = B[(TILE_SIZE * ph + ty) * col_result + col];\n",
        "            }\n",
        "            else{\n",
        "                B_ds[ty][tx] = 0.0;\n",
        "            }\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "            for (int k=0; k < TILE_SIZE; ++k){\n",
        "                dot_product[coarse_idx] += A_ds[ty][k] * B_ds[k][tx];\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "    for (int coarse_idx=0; coarse_idx<COARSE_FACTOR; ++coarse_idx ) {\n",
        "        int col = col_offset + TILE_SIZE * coarse_idx + tx;\n",
        "        if (row < rows_result && col < col_result ){\n",
        "            result[row * col_result + col] += dot_product[coarse_idx];\n",
        "        }\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "float* matrix_multiplication(float *h_a, float *h_b, int row_a, int col_a, int row_b, int col_b){\n",
        "    float *d_a, *d_b, *d_result;\n",
        "    int size_a = sizeof(float) * row_a * col_a;\n",
        "    int size_b = sizeof(float) * row_b * col_b;\n",
        "    int size_result = sizeof(float) * row_a * col_b;\n",
        "    float *h_result = new float[size_result];\n",
        "\n",
        "    //Allocate device memory\n",
        "    cudaError_t err = cudaMalloc((void**) &d_a, size_a);\n",
        "    CHECK_CUDA_CALL(err);\n",
        "    err = cudaMalloc((void**) &d_b, size_b);\n",
        "    CHECK_CUDA_CALL(err);\n",
        "    err = cudaMalloc((void**) &d_result, size_result);\n",
        "    CHECK_CUDA_CALL(err);\n",
        "\n",
        "    //copy matrices to device\n",
        "    err = cudaMemcpy(d_a, h_a, size_a, cudaMemcpyHostToDevice);\n",
        "    CHECK_CUDA_CALL(err);\n",
        "    err = cudaMemcpy(d_b, h_b, size_b, cudaMemcpyHostToDevice);\n",
        "    CHECK_CUDA_CALL(err);\n",
        "\n",
        "\n",
        "    int thread_x = TILE_SIZE;\n",
        "    int thread_y = TILE_SIZE;\n",
        "    dim3 block_dims(thread_x, thread_y, 1);\n",
        "    int blocks_x = (col_b + thread_x * COARSE_FACTOR - 1)/(thread_x * COARSE_FACTOR);\n",
        "    int blocks_y = (row_a + thread_y - 1)/thread_y;\n",
        "    dim3 grid_dims(blocks_x, blocks_y, 1);\n",
        "\n",
        "\n",
        "    matrix_multiplication_tiled_thread_coarsed<<<grid_dims, block_dims>>>(d_a, d_b, d_result, row_a, col_b, col_a);\n",
        "\n",
        "    // copy result to host\n",
        "    err = cudaMemcpy(h_result, d_result, size_result, cudaMemcpyDeviceToHost);\n",
        "    CHECK_CUDA_CALL(err);\n",
        "\n",
        "    //free device memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    return h_result;\n",
        "}\n",
        "\n",
        "\n",
        "void test_matrix_multiplication(){\n",
        "    cout << \"Running Test 1:\\n\";\n",
        "    float *A = new float[6];\n",
        "    float *B = new float[8];\n",
        "\n",
        "    fill_n(A, 6, 1.0f);\n",
        "    fill_n(B, 8, 1.0f);\n",
        "\n",
        "    float *C = matrix_multiplication(A, B, 3, 2, 2, 4);\n",
        "\n",
        "    for (int i=0; i< 3; ++i){\n",
        "        for (int j=0; j<4; ++j){\n",
        "            cout << C[i * 4 + j] << \" \";\n",
        "        }\n",
        "        cout << \"\\n\";\n",
        "    }\n",
        "\n",
        "\n",
        "    cout << \"\\nRunning test 2:\\n\";\n",
        "\n",
        "    A = new float[50*50];\n",
        "    B = new float[2500];\n",
        "\n",
        "    for (int i=0; i < 50; ++i){\n",
        "        A[i*50 + i] = 1;\n",
        "        B[i*50 + i] = 1;\n",
        "    }\n",
        "    C = matrix_multiplication(A, B, 50, 50, 50, 50);\n",
        "    for (int i=0; i< 50; ++i){\n",
        "        for (int j=0; j<50; ++j){\n",
        "            cout << C[i * 50 + j] << \" \";\n",
        "        }\n",
        "        cout << \"\\n\";\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "    test_matrix_multiplication();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd1Dea3-a-I0",
        "outputId": "b6dbd294-4e60-4cd5-929b-eebf6ac3f858"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_multiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3-pybind11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUMZoH6fbHgs",
        "outputId": "a19af59b-ced1-4163-b3cb-74467f523df1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3-pybind11 is already the newest version (2.9.1-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o matmul matrix_multiplication.cu"
      ],
      "metadata": {
        "id": "w-gJmX1TbmqM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DkNaFfkgjxd",
        "outputId": "2d02efef-d6b2-4f0c-d7ab-ea5f5c66ed3c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Test 1:\n",
            "2 2 2 2 \n",
            "2 2 2 2 \n",
            "2 2 2 2 \n",
            "\n",
            "Running test 2:\n",
            "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
            "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGC6fUGsjdRp",
        "outputId": "646d2afe-0d19-48fc-8a47-56e84394fbb7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/422.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_multiplication.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <torch/extension.h>\n",
        "#include <torch/types.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define STRINGFY(str) #str\n",
        "#define TORCH_BINDING_COMMON_EXTENSION(func) \\\n",
        "  m.def(STRINGFY(func), &func, STRINGFY(func));\n",
        "\n",
        "\n",
        "#define CHECK_CUDA_CALL(err)                                                \\\n",
        "    {                                                                       \\\n",
        "        if (err != cudaSuccess)                                             \\\n",
        "        {                                                                   \\\n",
        "            fprintf(stderr, \"CUDA error in file %s at line %d: %s\\n\",       \\\n",
        "                    __FILE__, __LINE__, cudaGetErrorString(err));           \\\n",
        "            exit(EXIT_FAILURE);                                             \\\n",
        "        }                                                                   \\\n",
        "    }\n",
        "\n",
        "#define TILE_SIZE 32\n",
        "#define COARSE_FACTOR 4\n",
        "// Reduce number of blocks and lower number of blocks can run parallely - few threads handle more \"units\" of work\n",
        "\n",
        "__global__\n",
        "void matrix_multiplication_tiled_thread_coarsed(float *A, float* B, float *result, int rows_result, int col_result, int inner_dim){\n",
        "    __shared__ float A_ds[TILE_SIZE][TILE_SIZE];\n",
        "    __shared__ float B_ds[TILE_SIZE][TILE_SIZE];\n",
        "\n",
        "    int bx = blockIdx.x; int tx = threadIdx.x;\n",
        "    int by = blockIdx.y; int ty = threadIdx.y;\n",
        "\n",
        "    int row = by * TILE_SIZE + ty;\n",
        "    int col_offset = bx * TILE_SIZE * COARSE_FACTOR;\n",
        "\n",
        "    float dot_product[COARSE_FACTOR];\n",
        "    for (int i=0; i< COARSE_FACTOR; ++i){\n",
        "        dot_product[i] = 0.0;\n",
        "    }\n",
        "    // Loop over all tiles\n",
        "    for (int ph=0; ph < (inner_dim + TILE_SIZE - 1)/TILE_SIZE; ++ph ){\n",
        "\n",
        "        //r = row, c = TILE_SIZE * ph + tx;\n",
        "        if (row < rows_result && (TILE_SIZE*ph + tx) < inner_dim){\n",
        "            A_ds[ty][tx] = A[row * inner_dim + TILE_SIZE * ph + tx];\n",
        "        }\n",
        "        else{\n",
        "            A_ds[ty][tx] = 0.0;\n",
        "        }\n",
        "\n",
        "        for (int coarse_idx=0; coarse_idx<COARSE_FACTOR; ++coarse_idx){\n",
        "            int col = col_offset + TILE_SIZE * coarse_idx + tx;\n",
        "            //row = TILE_SIZE * ph + ty, c = col\n",
        "            if ((TILE_SIZE * ph + ty) < inner_dim && col < col_result){\n",
        "                B_ds[ty][tx] = B[(TILE_SIZE * ph + ty) * col_result + col];\n",
        "            }\n",
        "            else{\n",
        "                B_ds[ty][tx] = 0.0;\n",
        "            }\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "            for (int k=0; k < TILE_SIZE; ++k){\n",
        "                dot_product[coarse_idx] += A_ds[ty][k] * B_ds[k][tx];\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "    for (int coarse_idx=0; coarse_idx<COARSE_FACTOR; ++coarse_idx ) {\n",
        "        int col = col_offset + TILE_SIZE * coarse_idx + tx;\n",
        "        if (row < rows_result && col < col_result ){\n",
        "            result[row * col_result + col] += dot_product[coarse_idx];\n",
        "        }\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "torch::Tensor matrix_multiplication(torch::Tensor A, torch::Tensor B) {\n",
        "    // Check inputs are CUDA tensors\n",
        "    TORCH_CHECK(A.device().is_cuda(), \"A must be a CUDA tensor\");\n",
        "    TORCH_CHECK(B.device().is_cuda(), \"B must be a CUDA tensor\");\n",
        "    TORCH_CHECK(A.dtype() == torch::kFloat32, \"A must be float32\");\n",
        "    TORCH_CHECK(B.dtype() == torch::kFloat32, \"B must be float32\");\n",
        "\n",
        "    int rows_a = A.size(0);\n",
        "    int inner_dim = A.size(1);\n",
        "    int cols_b = B.size(1);\n",
        "\n",
        "    auto result = torch::zeros({rows_a, cols_b}, A.options());\n",
        "\n",
        "    dim3 grid_dim(\n",
        "        (cols_b + TILE_SIZE * COARSE_FACTOR - 1) / (TILE_SIZE * COARSE_FACTOR),\n",
        "        (rows_a + TILE_SIZE - 1) / TILE_SIZE\n",
        "    );\n",
        "    dim3 block_dim(TILE_SIZE, TILE_SIZE);\n",
        "\n",
        "    matrix_multiplication_tiled_thread_coarsed<<<grid_dim, block_dim>>>(\n",
        "        A.data_ptr<float>(),\n",
        "        B.data_ptr<float>(),\n",
        "        result.data_ptr<float>(),\n",
        "        rows_a,\n",
        "        inner_dim,\n",
        "        cols_b\n",
        "    );\n",
        "\n",
        "    // Check for kernel launch errors\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    CHECK_CUDA_CALL(err);\n",
        "\n",
        "    return result;\n",
        "}\n",
        "\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  TORCH_BINDING_COMMON_EXTENSION(matrix_multiplication)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kYhLr1GgqnK",
        "outputId": "08ec8622-db74-4a39-c9d3-2be44862d5cf"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_multiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.cpp_extension import load\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)\n",
        "\n",
        "lib = load(\n",
        "    name=\"matrix_multiplication\",\n",
        "    sources=[\"/content/matrix_multiplication.cu\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asgxnXgbjS8s",
        "outputId": "11346524-0a01-43b9-fa60-d61f43532737"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n",
            "True\n",
            "12.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.ones(50, 50, dtype=torch.float32).cuda()\n",
        "B = torch.ones(50, 50, dtype=torch.float32).cuda()\n",
        "start_time = time.time()\n",
        "C = lib.matrix_multiplication(A, B)\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "NBpgVSJilnmo"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRyKQPbsvGlT",
        "outputId": "e61bbb84-3e66-419d-ad3a-14031fbcd484"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50., 50., 50.,  ..., 50., 50., 50.],\n",
            "        [50., 50., 50.,  ..., 50., 50., 50.],\n",
            "        [50., 50., 50.,  ..., 50., 50., 50.],\n",
            "        ...,\n",
            "        [50., 50., 50.,  ..., 50., 50., 50.],\n",
            "        [50., 50., 50.,  ..., 50., 50., 50.],\n",
            "        [50., 50., 50.,  ..., 50., 50., 50.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.eye(50, 50, dtype=torch.float32).cuda()\n",
        "B = torch.eye(50, 50, dtype=torch.float32).cuda()\n",
        "start_time = time.time()\n",
        "C = lib.matrix_multiplication(A, B)\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "UKbqz88dvHmV"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCUrQnOKvLzN",
        "outputId": "b14fba89-cad7-4e61-b2cf-29651cc9775f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87cprw-0vQmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}